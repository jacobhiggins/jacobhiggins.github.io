---
title: 'Sensor Fusion Using a Kalman Filter'
date: 2020-12-31
permalink: /posts/2020/12/31/blog-post/
mathjax: true
tags:
  - Kalman filter
  - Sensor fusion
---

### How do you fuse measurements together?

The first time you learn about probabilistic robotics, you will probably hear about the [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter). The Kalman filter is a way of estimating the state of a system that has both process noise and measurement noise. Founded in probability theory, it gives an optimal estimate based on the relative size of the process and measurement noise. If the process noise is very large and the measurement noise is very small, then the Kalman filter returns an estimated state that is closer to the measurement, and vice versa. For most students that first encounter the Kalman filter, you're told the intuition, shown some complicated multivariate Gaussian math and are told to use it in a homework exercise.

Most introductory examples of the Kalman filter have only one measurement to use. What if there are more than one, though? How do you handle two different measurements of the same exact thing with a Kalman filter? This is form of sensor fusion, and when I first learned the Kalman filter I had solve this problem. The purpose of this blog post is to show how the Kalman filter can perform sensor fusion, and hopefully clarify the machinery of the Kalman filter in the process.

### The two steps of the Kalman filter

Let us define $p(\mathbf{x})$ as the belief probability distribution of state space vector $\mathbf{x}$. A Kalman filter has two important steps when providing an estimate of the value of $\mathbf{x}$:
 - Prediction update step (use input $\mathbf{u}$ to update $p(\mathbf{x}$)
 - Measurement update step (use measurement $\mathbf{y}$ to update $p(\mathbf{x}$)

Since the Kalman filter assumes multivariate Gaussian probability distributions, only two quantities are recorded over each step $k$ in the Kalman filter: an estimate vector $\hat{\mathbf{x}_k}$ (corresponding to the mean of the Gaussian), and a covariance matrix $P\_k$ (corresponding to the confidence of the measurement). Together, these quantities fully define the probability distribution $p(\mathbf{x})$, so when the Kalman filter updates $p(\mathbf{x})$ in the prediction and the measurement step, it really just updates these two variables.

What if we have two separate measurements $\mathbf{y}^a$ and $\mathbf{y}^b$, modeled with their own Gaussian noise? Cutting to the punchline, you perform _two_ measurement steps, one with each measurement:
 - Prediction update step (use input $\mathbf{u}$ to update $p(\mathbf{x}$)
 - First measurement update step (use measurement $\mathbf{y}^a$ to update $p(\mathbf{x}$)
 - Second measurement update step (use measurement $\mathbf{y}^b$ to update $p(\mathbf{x}$)

This seems like a natural thing to do given two different measurements. Let's see why this is so.

### The Bayes' Update

The Kalman filter is a specific type of filter called a Bayes' filter. The Bayes' filter also has two steps: one prediction step and one measurement step. In order to change the Kalman filter to incorporate more than one measurement step, we need to understand what each step means in terms of Bayesian estimation. 

Define the belief distribution at iteration $k-1$ as $p\_{k-1|k-1}(\mathbf{x})$. The $k-1|k-1$ part can be read as the probability distribution at time $k-1$, given information up to time $k-1$. The prediction step requires some process model that describes the probability of reaching state $\mathbf{x}^+$ given current state $\mathbf{x}$ and input $\mathbf{u}_{k-1}$:

\begin{equation\*}
p(\mathbf{x}^+|\mathbf{x},\mathbf{u}_{k-1})
\end{equation\*}

The prediction update is thus described in the Bayes' filter as:

\begin{equation}
p(\mathbf{x})\_{k|k-1} = \sum\_{\mathbf{x}'} p(\mathbf{x}|\mathbf{x}',\mathbf{u}_{k-1}) p\_{k-1|k-1}(\mathbf{x})
\end{equation}

At time step $k$, we then receive two measurements $\mathbf{y}^a_k$ and $\mathbf{y}^b_k$. The measurement update of the Bayes' filter is then used to find the probability distribution of the state given these two measurements, or $p\_{k|k}(\mathbf{x})=p(\mathbf{x}|$\mathbf{y}^a_k$,$\mathbf{y}^b_k$)$. In the case of a single measurement, Bayes' rule is used to "switch" the random variable and the conditioned variable:

\begin{equation}

\end{equation}