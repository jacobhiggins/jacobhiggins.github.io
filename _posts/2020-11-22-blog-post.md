---
title: 'Exploring Solutions of the Hamilton-Jacobi-Bellman Equation'
date: 2020-11-22
permalink: /posts/2020/11/blog-post/
mathjax: true
tags:
  - Optimal Control Theory

---

### Does the HJB equation really find the optimal control law?

In optimal control theory, the Hamilton-Jacobi-Bellman equation is a PDE that gives a necessary and sufficient condition with respect to a cost function. In other words, if we can solve the HJB equation, then we find _the_ optimal control law. In most examples detailing how the HJB equation is solved, the discussion stops as soon as the answer is found. This is usually fine, but I've always wondered if the answer we find is actually the optimal answer. Can we explore the optimality of HJB equation solutions using graphs?

In this blog post, I'll walk through the basic steps required to find the optimal control law with the HJB equation. After, I'll play around with optimal solution by graphing solutions that are perturbed by small amounts, showing graphically how these solutions optimize the cost function.

### Solving the HJB equation

Suppose we define the following cost function:

\begin{equation}
J(x(t),t) = \int_t^T g(x(\tau),u(\tau) d\tau + h(x(T),u(T))
\end{equation}

Here, $g(x,u)$ is a (usually positive definite) function that describes the instantaneous cost that $J(x,u)$ accrues at time $t$. Similarly, $h(x,u)$ describes the final cost at terminal time $T$. Notice how in this context, the cost function is a function of time $t$, and describes the future costs accrued until $T$. This is a consequence of [Bellman's principle of optimality](https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_principle_of_optimality). I'll spare you the long explanation, as a lot of other people have already covered it in many different contexts. The basic idea, though, is if I want to achieve a certain state in an optimal way, I should work backwards from that desired state to any other feasible state. By finding an optimal strategy as you work backwards, this must be _the_ optimal strategy if you were to move forward instead. The cost function is also a function of the current state $x(t)$. 

For a given starting state $x_0$, future states evolve according to some differential equation:

\begin{equation}
\dot{x} = f(x,u)
\end{equation}

Optimal control theory seeks a control law $u := u(x)$ such that the cost $J$ is minimized. This is where the HJB equation comes into play. It says that if we find a control law that satisfies the following PDE:

\begin{equation}
J\_t(x,t) + \min\_u\left{ g(x,t) + J\_x \cdot f(x,u) \right}
\end{equation}

Then we find the optimal control law for the associated cost function. This PDE has the following boundary condition:

\begin{equation}
J(x(T),T) = h(x(T),T)
\end{equation}

Again because of the principle of optimality, our boundary condition is placed at the terminal time $T$.

In order to get any further, we have to now define our system and cost function. Let's define the system dynamics. To keep things simple, I'll stick with a very basic LTI one-dimensional system:

\begin{equation}
\dot{x} = ax + bu
\end{equation}

Now let's define a reasonable cost function as:

\begin{equation}
J(x(t),t) = \int_t^T cx(\tau)^2 + du(\tau)^2 d\tau + 0.5fx(T)
\end{equation}

The coefficients $a$ through $f$ are kept general for now. With these definitions, we can start to actually solve the HJB equation.

First, lets focus on the following term:

\begin{equation*}

\end{equation*}