---
title: 'Entropy as Information'
date: 2020-12-23
permalink: /posts/2020/12/blog-post/
mathjax: true
tags:
  - Shannon Entropy
  - Information
  - Occupancy Mapping
---

### How is information quantified?

This is a big question that I had when first heard about [information theory](https://en.wikipedia.org/wiki/Information_theory). When you or I think of words with a lot of information, we might imagine a juicy bit of gossip spread around office, or maybe a very interesting news article describing the latest political upset. But this kind of information is really subjective, meaning what is a lot of information for one person is old news to another. How do you quantify something as ambiguous as information?

First I'll say that in information theory, information is _not_ about the meaning behind the words. In fact, the words could be literally meaningless. They don't even have to be words! Sure, we work with things that do ultimately have meanings behind them, but information theory doesn't care about that. What does information theory care about then? In a nutshell, "surprise." Information theory tries to quantify how surprised you are by a certain outcome. If you think about it, information and surprise are related in that if you are surprised by something, that something probably contains a good piece of information. Again, though, "surprise" is subjective. How can you quantify these ideas?

Information theory relies on the probability of something (or multiple things) happening in order to quantify surprise. If something is unlikely to happen, then you would be surprise if that thing happens. But if something is very unlikely, then it is very unlikely that you'll be surprised. This notion is capture by entropy, which itself is a measure of uncertainty of a system. In this blog post, I talk about how I like to think of entropy and how it relates to information theory. Later, I'll discuss how these ideas can be applied to autonomous exploration.

### A crystal ball that predicts the weather

<p align="center">
  <img width="460" height="300" src="/images/blog_pics/2020/EntropyAsInformation/crystalball.png">
</p>

Imagine your friend asks you for a crystal ball for Christmas; specifically, this crystal should predict if it will rain or not rain today. This is an example of a binary predictor, where there are two mutually exclusive options to choose from. When your friend queries this crystal ball, it gives them a prediction of rain or no rain. For now, we'll just look at the probability that the crystal ball is correct, $P(correct)$.

You don't really like your friend, so you want to get them a crystal ball that produces the worst results possible. What would that be? Well, the _best_ results would be a crystal ball that is always right, or $P(correct)=1$. You might reason that the worst result would then be a crystal ball that always gives the wrong answer, or $P(correct)=0$. Here is a table of what the completely wrong crystal ball might predict over the course of five days:

<p align="center">
  <img width="460" height="300" src="/images/blog_pics/2020/EntropyAsInformation/wrong_predictions.png">
</p>

At first, you're friend might be really peeved that they keep getting caught in the rain without an umbrella, but after they dry off they might realize that they can still use the crystal ball: if it is always wrong, then they just assume the _opposite_ prediction that it gives to get the right answer. If it says it will rain, then your friend knows its a sunny day, and if it says there will be no rain that day, they better bring a coat!

Imagine taking a multiple choice test where each question has only two options. If you're looking to always be wrong on each question, you have to know every right answer in one way or another. From the perspective of information theory, always being wrong is equivalent to always being right, since you need the same information to do either. But if this is the case, what makes the worst crystal ball?

The worst gift would be a crystal ball that is sometimes right, sometimes wrong. In fact, you're intuition might be telling you that the worst case scenario is a crystal ball that is correct only $50\%$ of the time, and you would be correct. Think about it: if the crystal ball was, say, $90\%$ correct, then you know that an average you could trust what it tells you. But if the crystal ball was right only half the time, then you wouldn't be able to tell at all if it will rain that day, not even on average. In other words, the crystal ball that has $P(correct)=P(incorrect)=0.5$ cannot reliably give you any information, and you will always be surprised by what the weather is when you actually walk outside. If instead $P(correct)=1$ or $P(incorrect)=1$, then you will never be surprised. 

### Information Content and Shannon Entropy

How can we further strengthen these notions of surprise when dealing with random variables like the weather? For a single outcome, we just saw that the amount of surprise for that outcome is related to its probability $p_i$. In information theory, surprise is quantified by something called the information content $I$, and is defined by the random variable $X$ and a particular outcome of that variable $O$, along with the probability of that outcome $p\_O$:

\begin{equation}
I\_X(O) \equiv -\ln(p\_O)
\end{equation}

There is no strict derivation for this quantity, it has no physical meaning, and in a sense it is completely made-up by humans. But, it _does_ have nice properties that make it ideal to work with. We won't discuss them all here, but notice how if $p\_O=1$, then $I\_X(O)=0$. In English, if a particular outcome is certain to occur, then we are not at all surprised by the outcome of this random variable. On the other hand, if $p\_O\rightarrow 0$, then $I\_X(O)\rightarrow\infty$. This means that as an event becomes more and more unlikely to occur, then we become more and more surprised if we actually see that outcome.

In our crystal ball example, we can track its information content by considering whether its weather prediction is correct $p\_c$ or incorrect $p\_{~c}$. If $p\_c=1$, then we are not at all surprised if the crystal ball correctly predicts the weather $I(c)=0$, but will be very surprised if it is incorrect $I(~c)=\infty$. Since the information content is tied to the outcome of a random variable, we cannot talk about the information content of the crystal ball; we can only state the information content of the crystal ball _when it gives us a particular outcome_. How can we talk about the crystal ball without referencing a certain outcome?

The answer comes by averaging over all possible outcomes, to find the average information content provided by the crystal ball. The average information content is given by a term that is call the entropy $H(X)$ that is tied to random variable $X$:

\begin{equation}
H(X) = -\sum\_Op\_O\ln(p\_O)
\end{equation}

The information entropy quantifies how much information can be gleaned from a random variable $X$ on average. For a binary random variable and assuming both outcomes are mutually exclusive (like our crystal ball), the entropy is given by a sum of two terms:

\begin{equation}
H = -p\_c\ln(p\_c) - (1-p\_c)\ln(1-p\_c)
\end{equation}

Here is a graph of the entropy as a function of $p\_c$:

<p align="center">
  <img width="460" height="300" src="/images/blog_pics/2020/EntropyAsInformation/binary_entropy.png">
</p>

The entropy of the crystal ball approaches zero when $p\_c=1$ and $p\_c=0$. In these scenarios, we are not surprised at the weather when we walk outside because the crystal ball told us, and the crystal ball is very accurate. If $p\_c=0.5$, we are maximally surprised by the weather because the crystal ball is maximally inaccurate.

### Using Entropy of Autonomous Exploring

This idea of entropy is used a lot in autonomous vehicle motion planning. In particular, it is used when a robot doesn't know about its surroundings and must explore to create a map that it can use. Suppose that a robot is tasked with observing the occupancy of room A and room B. Somehow, it already knows the occupancy of room B (either it was given this information, or it had already observed the occupancy of that room). The robot must choose which room to go to:

<p align="center">
  <img width="460" height="300" src="/images/blog_pics/2020/EntropyAsInformation/entropy_exploration.png">
</p>

The obvious choice is room A, since observing room B wouldn't result in any net increase in information. In the context of information theory, we would say that the probability of occupancy of room B is either $p_B=1$ or $p_B=0$, depending on if the room is empty or not (in the above figure, room B is occupied by a pupper). In either case, the entropy of room B is zero. The probability of occupancy of room A on the other hand would be $p_A=0.5$, since we are completely uncertain if the 