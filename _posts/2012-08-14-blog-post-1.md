---
title: 'Reinforcement Learnin 1: Policy Iteraction, Value Iteration and the Frozen Lake'
date: 2020-06-22
permalink: /posts/2020/06/blog-post-1/
mathjax: true
tags:
  - reinforcement learning

---

Introduction
======

Reinforcement learning as a whole is concerned with learning how to behave to get the best outcome given a situation. Although there are many areas of application, the most well-known is videogames. Given where you are in the virtual world and the position of the enemies around you, what's the best action to take? Should you walk forward or jump on the platform above? These questions are split-second decisions for humans, but are non-trivial for a computer to figure out.

In practice, reinforcement learning operates a lot like how you are I might learn to play a videogame. We give the computer goals to achieve and things to avoid, and it takes many attempts (called "episodes") to figure out how to play the game; if theres an enemy in front, jump, else move forward. Codifying these ideas into a mathematical framework is the major idea behind reinforcement learning.

In this post, I'll review the basic ideas behind reinforcement learning and discuss two basic algorithms - policy iteration and value iteration. I'll also explain how to use OpenAI Gym, a popular python package used for testing different RL algorithms. With it, I'll use policy iteration and value iteration to teach a computer how to walk on a frozen lake.

The Best Action to Take: The Foundation of RL
======

So, how does a computer know what to do in a given situation? In order to answer that question, we need to set the stage and define the terminology of reinforcement learning.

A state 

$$ s_t $$