---
title: 'Sensor Fusion Using a Kalman Filter'
date: 2020-12-31
permalink: /posts/2020/12/31/blog-post/
mathjax: true
tags:
  - Kalman filter
  - Sensor fusion
---

### How do you fuse measurements together?

The first time you learn about probabilistic robotics, you will probably hear about the [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter). The Kalman filter is a way of estimating the state of a system that has both process noise and measurement noise. Founded in probability theory, it gives an optimal estimate based on the relative size of the process and measurement noise. If the process noise is very large and the measurement noise is very small, then the Kalman filter returns an estimated state that is closer to the measurement, and vice versa. For most students that first encounter the Kalman filter, you're told the intuition, shown some complicated multivariate Gaussian math and are told to use it in a homework exercise.

Most introductory examples of the Kalman filter have only one measurement to use. What if there are more than one, though? How do you handle two different measurements of the same exact thing with a Kalman filter? This is form of sensor fusion, and when I first learned the Kalman filter I had solve this problem. The purpose of this blog post is to show how the Kalman filter can perform sensor fusion, and hopefully clarify the machinery of the Kalman filter in the process.

### The two steps of the Kalman filter

Let us define $p(\mathbf{x})$ as the belief probability distribution of state space vector $\mathbf{x}$. A Kalman filter has two important steps when providing an estimate of the value of $\mathbf{x}$:
 - Prediction update step (use input $\mathbf{u}$ to update $p(\mathbf{x}$)
 - Measurement update step (use measurement $\mathbf{y}$ to update $p(\mathbf{x}$)

Since the Kalman filter assumes multivariate Gaussian probability distributions, only two quantities are recorded over each step $k$ in the Kalman filter: an estimate vector $\hat{\mathbf{x}_k}$ (corresponding to the mean of the Gaussian), and a covariance matrix $P\_k$ (corresponding to the confidence of the measurement). Together, these quantities fully define the probability distribution $p(\mathbf{x})$, so when the Kalman filter updates $p(\mathbf{x})$ in the prediction and the measurement step, it really just updates these two variables.

What if we have two separate measurements $\mathbf{y}^a$ and $\mathbf{y}^b$, modeled with their own Gaussian noise? Cutting to the punchline, you perform _two_ measurement steps, one with each measurement:
 - Prediction update step (use input $\mathbf{u}$ to update $p(\mathbf{x}$)
 - First measurement update step (use measurement $\mathbf{y}^a$ to update $p(\mathbf{x}$)
 - Second measurement update step (use measurement $\mathbf{y}^b$ to update $p(\mathbf{x}$)


