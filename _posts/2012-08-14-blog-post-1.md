---
title: 'Reinforcement Learning 1: Policy Iteraction, Value Iteration and the Frozen Lake'
date: 2020-06-22
permalink: /posts/2020/06/blog-post-1/
mathjax: true
tags:
  - reinforcement learning

---

Introduction
======

Reinforcement learning as a whole is concerned with learning how to behave to get the best outcome given a situation. Although there are many areas of application, the most well-known is videogames. Given where you are in the virtual world and the position of the enemies around you, what's the best action to take? Should you walk forward or jump on the platform above? These questions are split-second decisions for humans, but are non-trivial for a computer to figure out.

In practice, reinforcement learning operates a lot like how you are I might learn to play a videogame. We give the computer goals to achieve and things to avoid, and it takes many attempts (called "episodes") to figure out how to play the game; if there's an enemy ahead, jump, else move forward. Codifying these ideas into a mathematical framework is the major idea behind reinforcement learning.

In this post, I'll review the basic ideas behind reinforcement learning and discuss two basic algorithms - policy iteration and value iteration. I'll also explain how to use OpenAI Gym, a popular python package used for testing different RL algorithms. With it, I'll use policy iteration and value iteration to teach a computer how to walk on a frozen lake.

The next section is a very brief overview of basic concepts in RL. Although they aren't difficult, there are many nuances and side discussions that are worth having but aren't included for brevity. The reader is directed to [Sutton and Barro's Introduction to Reinforcement Learning](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf), a text that deservedly holds a place as the first text almost everyone encounters when first learning about RL. The notation I use in this post is borrowed from that book.

The Best Action to Take: The Foundation of RL
======

So, how does a computer know what to do in a given situation? In order to answer that question, we need to set the stage and define the terminology of reinforcement learning.

A state $s_t$ defines the what the player and/or environment looks like at time $t$. For example, $s_t$ might describe the position the player is at, or where the enemies are located. In this state, there is a set of actions that the player can take. Go left? Go right? Jump? Each action that a player can take is denoted as $a$.

If we are in state $s$ and decide to take action $a$, we are taken to some state $s'$ (which could, generally speaking, be the same state $s$). The mathematical description of what takes us from state $s$ to state $s'$ under action $a$ called the _model_. In some sense, one can think of the model as the equations of motion. If I am sitting in a car at rest (state $s$) and put the pedal to the metal (action $a$), then my car move forward and increase its speed (new state $s'$).

In general, there is some probability of entering state $s'$ from state $s$ under action $a$. This probability can be described as:

\begin{align}
\label{eq:transition_noreward}
p(s'|s,a)
\end{align}

For those that might be unfamiliar with this notation, the above notation says: given some current state $s$ and action $a$ (terms to the right of the bar), what is the probabilty to transitioning to state $s'$ (terms to the let of the bar)? These transition probabilities describe a Markov Decision Process (MDP). An MDP is much like a Markov chain, accept the probability are also determined by an action that we (or a computer that plays for us) must choose. From a state $s$, choosing an action $a$ determines the probabilities and states $s'$ to which our system may transition.

Usually, we choose an action based on our current state $s$. And, just like humans, RL algorithms keep track of a strategy, or _policy_, that it uses when it encounters a state. This policy is denoted by $\pi(a\|s)$, which is the probability of choosing action $a$ given that we are in the current state $s$.

Aside: one question you might have is why we bother to define probabilities state-action $(s,a)$ to new state $s'$, $p(s'\|s,a)$, and probabilities from state to action, $\pi(a\|s)$ separately; why not combine them as $\sum_a p(s'\|s,a)\pi(a\|s) = p(s'\|s)$, summing over all possible actions from state $s$ to state $s'$ and eliminating this action variable? Well for one, usually only a single action $a$ can take you from $s$ to $s'$, so that sum reduces to a single term. But the bigger idea is that these two probabilities describe two very different things. The transition probabilties $p(s'\|s,a)$ describe how the world works, and is largely out of our control. What actions we take, however, are in our control. In RL, finding the best policy $\pi(a\|s)$ is the goal, or in other words, RL seeks to find the best action to take for any state $s$ we might find ourselves in.

The last piece of the puzzle is telling the computer when it does something good, and when it does something bad. This is done through the use of _rewards_. Rewards are typically define by certain states that the system can achieve. For Mario, his goal is to reach the end of the level, so when he reaches the end, we might give him a positive reward. If Mario hits a goomba or falls in a hole, we might give him a negative reward. Mario would then associate the actions that led him to his most recent outcome with a positive reward (if he completed the level) or a negative reward (if he died). This would help him update his policy for achieve a higher reward or help him avoid a negative reward. 

Rewards are incorporated into the transition probabilities define above, so Eq. \ref{eq:trans_noreward} is altered only slightly:

\begin{align}
\label{eq:trans_prob}
p(s',r|s,a)
\end{align}

Eq. \ref{eq:trans_prob} is read as the probability of transitioning to state $s'$ and recieving reward $r$, starting in state $s$ and applying action $a$. As mentioned before, rewards are usually associated with states $s'$. Positive rewards are associated with desired states (e.g. goals in a game) and negative rewards are associated with undesired states (e.g. hitting a goomba). Other than that general guideline, rewards can be defined as anything we might want. In general, the policy that the RL algorithm ultimately learns is dependent on how we define the rewards, and there common cases where poorly-defined rewards cause undesired behavior. But that is something we can explore later.

With this whole system of rewards and transitions in place, what exactly do we want the RL algorithm to do? Every time the policy chooses an action and the system transitions from one state to another, the computer gets some reward $r$. This reward is cumulative, meaning each step in time gains some reward $R_{t+1} = R_t + r$, where we usually initialize $R_0=0$. The total reward is then dependent on the starting state and the subsequent actions that we took.

Suppose we are at time $t$ in the current game. If we are trying to decide what action to take next, one logical thing to do would be to try to look into the future and _maximize our future rewards_. If we currently have cumulative reward $R_t$, then the expected return $G_t$ is simply the sum of all future rewards:

\begin{align}
\label{eq:return_nodiscout}
G_t = R_{t+1} + R_{t+2} + ... + R_{T}
\end{align}

Here, $T$ is the maximum time of playing. Because each of these $R_{t+i}$ future rewards are generally stochastic variables, we would like to _maximize the expected return_. If there is no maximum time of playing $T$, the sum in infinite and there is sometimes a risk of a reward accumulating to infinity. In this case, the maximum expected return would also be infinite. Since infinites are usually difficult to work with mathematically, all future rewards from current time $t$ are "discounted" by a factor $\gamma$. The discounted return is defined by:

\begin{align}
\label{eq:discounted_return}
G_t = R_{t+1} + \gammaR_{t+2} + ... = \sum_{k=0}^\inf\gamma^kR_{t+k+1}
\end{align}

In order for this infinite sum to remain finite, the discount factor $\gamma\in[0,1]$. One can think of $\gamma$ as a factor that reduces the importance of rewards from the current state. If $\gamma=0$, then our strategy to maximize $G_t$ would be to take actions that immediately give us positive reward $r$. If $\gamma=1$, then our strategy would look at long-term rewards and perhaps would take a negative reward now for the chance of a bigger positive reward later.

Lastly, let's discuss value functions and action-value functions. A value function $v_\pi(s)$ is simply the expected value for the return, given a policy $\pi(a\|s)$, starting in state $s$:

\begin{align}
v_\pi(s) = \mathbb{E}_pi\left[ G_t | S_t=s\right] = \mathbb{E}_pi\left[ \sum_{k=0}^\inf \gamma^kR_{t+k+1}\right]
\end{align}

Here, $\mathbb{E}[ \dot ]$ is the expectation value of a stochastic variable. For anyone unfamiliar, the expectation value is exactly the same as the average value: if start in state $s$ a million times and use policy $\pi$ each time, what will be the average reward that we get? As you might guess, getting this value (or at least a good estimate of this value) involves the state transition probabilities.

The action-value function is defined in a similar way, except it describes the expected return if we start in state $s$, take action $a$, and then afterwards always follow the same policy $\pi$:

\begin{align}
q_\pi(s,a) = \mathbb{E}
\end{align}