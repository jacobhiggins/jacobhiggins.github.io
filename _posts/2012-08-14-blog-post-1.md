---
title: 'Reinforcement Learning 1: Policy Iteraction, Value Iteration and the Frozen Lake'
date: 2020-06-22
permalink: /posts/2020/06/blog-post-1/
mathjax: true
tags:
  - reinforcement learning

---

First Steps in Reinforcement Learning
======

Reinforcement learning as a whole is concerned with learning how to behave to get the best outcome given a situation. Although there are many areas of application, the most well-known is videogames. Given where you are in the virtual world and the position of the enemies around you, what's the best action to take? Should you walk forward or jump on the platform above? These questions are split-second decisions for humans, but are non-trivial for a computer to figure out.

In practice, reinforcement learning operates a lot like how you are I might learn to play a videogame. We give the computer goals to achieve and things to avoid, and it takes many attempts (called "episodes") to figure out how to play the game; if there's an enemy ahead, jump, else move forward. Codifying these ideas into a mathematical framework is the major idea behind reinforcement learning.

In this post, I'll review the basic ideas behind reinforcement learning and discuss two basic algorithms - policy iteration and value iteration. I'll also explain how to use OpenAI Gym, a popular python package used for testing different RL algorithms. With it, I'll use policy iteration and value iteration to teach a computer how to walk on a frozen lake.

The next section is a very brief overview of basic concepts in RL. Although they aren't difficult, there are many nuances and side discussions that are worth having but aren't included for brevity. The reader is directed to [Sutton and Barro's Introduction to Reinforcement Learning](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf), a text that deservedly holds a place as the first text almost everyone encounters when first learning about RL. The notation I use in this post is borrowed from that book.

The Best Action to Take: The Foundation of RL
======

So, how does a computer know what to do in a given situation? In order to answer that question, we need to set the stage and define the terminology of reinforcement learning.

A state $s_t$ defines the what the player and/or environment looks like at time $t$. For example, $s_t$ might describe the position the player is at, or where the enemies are located. In this state, there is a set of actions that the player can take. Go left? Go right? Jump? Each action that a player can take is denoted as $a$.

If we are in state $s$ and decide to take action $a$, we are taken to some state $s'$ (which could, generally speaking, be the same state $s$). The mathematical description of what takes us from state $s$ to state $s'$ under action $a$ called the _model_. In some sense, one can think of the model as the equations of motion. If I am sitting in a car at rest (state $s$) and put the pedal to the metal (action $a$), then my car move forward and increase its speed (new state $s'$).

In general, there is some probability of entering state $s'$ from state $s$ under action $a$. This probability can be described as:

\begin{align}
\label{eq:transition_noreward}
p(s'|s,a)
\end{align}

For those that might be unfamiliar with this notation, the above notation says: given some current state $s$ and action $a$ (terms to the right of the bar), what is the probabilty to transitioning to state $s'$ (terms to the let of the bar)? These transition probabilities describe a Markov Decision Process (MDP). An MDP is much like a Markov chain, accept the probability are also determined by an action that we (or a computer that plays for us) must choose. From a state $s$, choosing an action $a$ determines the probabilities and states $s'$ to which our system may transition.

Usually, we choose an action based on our current state $s$. And, just like humans, RL algorithms keep track of a strategy, or _policy_, that it uses when it encounters a state. This policy is denoted by $\pi(a\|s)$, which is the probability of choosing action $a$ given that we are in the current state $s$.

Aside: one question you might have is why we bother to define probabilities state-action $(s,a)$ to new state $s'$, $p(s'\|s,a)$, and probabilities from state to action, $\pi(a\|s)$ separately; why not combine them as $\sum_a p(s'\|s,a)\pi(a\|s) = p(s'\|s)$, summing over all possible actions from state $s$ to state $s'$ and eliminating this action variable? Well for one, usually only a single action $a$ can take you from $s$ to $s'$, so that sum reduces to a single term. But the bigger idea is that these two probabilities describe two very different things. The transition probabilties $p(s'\|s,a)$ describe how the world works, and is largely out of our control. What actions we take, however, are in our control. In RL, finding the best policy $\pi(a\|s)$ is the goal, or in other words, RL seeks to find the best action to take for any state $s$ we might find ourselves in.

The last piece of the puzzle is telling the computer when it does something good, and when it does something bad. This is done through the use of _rewards_. Rewards are typically define by certain states that the system can achieve. For Mario, his goal is to reach the end of the level, so when he reaches the end, we might give him a positive reward. If Mario hits a goomba or falls in a hole, we might give him a negative reward. Mario would then associate the actions that led him to his most recent outcome with a positive reward (if he completed the level) or a negative reward (if he died). This would help him update his policy for achieve a higher reward or help him avoid a negative reward. 

Rewards are incorporated into the transition probabilities define above, so Eq. \ref{eq:trans_noreward} is altered only slightly:

\begin{align}
\label{eq:trans_prob}
p(s',r|s,a)
\end{align}

Eq. \ref{eq:trans_prob} is read as the probability of transitioning to state $s'$ and recieving reward $r$, starting in state $s$ and applying action $a$. As mentioned before, rewards are usually associated with states $s'$. Positive rewards are associated with desired states (e.g. goals in a game) and negative rewards are associated with undesired states (e.g. hitting a goomba). Other than that general guideline, rewards can be defined as anything we might want. In general, the policy that the RL algorithm ultimately learns is dependent on how we define the rewards, and there common cases where poorly-defined rewards cause undesired behavior. But that is something we can explore later.

With this whole system of rewards and transitions in place, what exactly do we want the RL algorithm to do? Every time the policy chooses an action and the system transitions from one state to another, the computer gets some reward $r$. This reward is cumulative, meaning each step in time gains some reward $R_{t+1} = R_t + r$, where we usually initialize $R_0=0$. The total reward is then dependent on the starting state and the subsequent actions that we took.

Suppose we are at time $t$ in the current game. If we are trying to decide what action to take next, one logical thing to do would be to try to look into the future and _maximize our future rewards_. If we currently have cumulative reward $R_t$, then the expected return $G_t$ is simply the sum of all future rewards:

\begin{align}
\label{eq:return_nodiscout}
G_t = R_{t+1} + R_{t+2} + ... + R_{T}
\end{align}

Here, $T$ is the maximum time of playing. Because each of these $R_{t+i}$ future rewards are generally stochastic variables, we would like to _maximize the expected return_. If there is no maximum time of playing $T$, the sum in infinite and there is sometimes a risk of a reward accumulating to infinity. In this case, the maximum expected return would also be infinite. Since infinites are usually difficult to work with mathematically, all future rewards from current time $t$ are "discounted" by a factor $\gamma$. The discounted return is defined by:

\begin{align}
\label{eq:discounted_return}
G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^\infty\gamma^kR_{t+k+1}
\end{align}

In order for this infinite sum to remain finite, the discount factor $\gamma\in[0,1]$. One can think of $\gamma$ as a factor that reduces the importance of rewards from the current state. If $\gamma=0$, then our strategy to maximize $G_t$ would be to take actions that immediately give us positive reward $r$. If $\gamma=1$, then our strategy would look at long-term rewards and perhaps would take a negative reward now for the chance of a bigger positive reward later.

Lastly, let's discuss value functions and action-value functions. A value function $v_\pi(s)$ is simply the expected value for the return, given a policy $\pi(a\|s)$, starting in state $s$:

\begin{align}
v_\pi(s) = E_\pi [ G_t | S_t=s ] = E_\pi \left[ \sum _{k=0}^\infty \gamma^k R _{t+k+1}|S_t=s \right] 
\end{align}

Here, $E[ \cdot ]$ is the expectation value of a stochastic variable. For anyone unfamiliar, the expectation value is exactly the same as the average value: if start in state $s$ a million times and use policy $\pi$ each time, what will be the average reward that we get? As you might guess, getting this value (or at least a good estimate of this value) involves the state transition probabilities.

The action-value function is defined in a similar way, except it describes the expected return if we start in state $s$, take action $a$, and then afterwards always follow the same policy $\pi$:

\begin{align}
q_\pi(s,a) = E_\pi [G_t | S_t=s, A_t=a ] = E_\pi\left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} | S_t=s, A_t=a\right]
\end{align}

Note that $v_\pi(s) = q_\pi(s,\pi(s))$.

The value function and the action-value function are essentially relative measures of the policy $\pi$. If $\pi$ is a terrible policy, then the value function will be small (probably zero or even negative) values for all states $s\in S$. There is at least one policy that is better that all the rest, producing the maximum value function and action value function. In RL, the goal then is to find the policy that maximizes the value function for all state values:

\begin{align}
v_*(s) = \text{max}_{\pi} v_pi(s)
\end{align}

Optimal policies also share the same optimal action-value function:

\begin{align}
q_*(s,a) = \text{max}_{\pi}q(s,a)
\end{align}

Our goal for this post is to find the optimal policy, given transition probabilities $p(s',r\|s,a)$ and reward values $r$. The two standard ways of doing so is with value iteration and policy iteration.

Value Iteration
======

We'll first start with value iteration, as I believe it is the easier to understand conceptually. I'll show the algorithm, then step through the first several iterations:

1. Start with model $p(s',r\|s,a)$
2. Initialize value function $v(s)=0$ for all states $s$
3. Initialize new value function $v'(s)=0$ to be used in do-while loop
4. do:
5. &emsp; for $s\in S$:
6. &emsp; &emsp; $v(s) = v'(s)$
7. &emsp; &emsp; $v'(s) = \text{max}_a \sum _{s'\in S} P(s',r\|s,a)\left( r + \gamma v(s') \right)$
8. while $\|v'(s) - v(s)\|_1 < \text{small number}$
9. return v(s)

In order to show what this algorithm does, lets look at a simple example. Below is a 1D world comprised of seven squares.  Our goal is to reach the rightmost square, and avoid the leftmost square. If we reach either, the game is over. In accordance with this goal, we set the following reward of +10 if we reach the right sqaure, and a reward of -1 if we reach the left square. All other state-action pairs result in a reward of zero. For our small world, let $\gamma=1$.

Inside each square is the value for $v(s)$, which we initialize to zero everywhere:

![Initial Value Function](/images/blog_pics/RL1/fig0.png)

Our action space is simple: we can move left, move right or stand still. Everytime we take an action, we are certain to complete that action. This means that, for our model, our probabilities are all ones or zeros. For example, $p(s'=4,r=0\|s=3,a=\text{right})=1$, and $p(s'=7,r=10\|s=2,a=\text{right})=0$.

Now let's go through the algorithm. We already initialized our value function to zero for all states (step 2), so now we enter the do-while loop. For all states, look at each action and the associated reward $r$ and discounted value $\gamma v(s')$. For the first iteration of this loop, this simply returns the rewards associated with the two end-game states:

![Value Function, First Iteration](/images/blog_pics/RL1/fig1.png)

Line 6 of the algorithm simply says look at the biggest change in the value function. If the biggest change is bigger than some small number, then continue the loop. In our case, square 7 saw a change of +10. So we continue the loop.

Because states 1 and 7 are end-game states, their values cannot change - we can't get any more reward, because the game has ended! So we'll focus on the other states and see how they change.

Below is a list of how the value of each state is updated on the second iteration:

- State 2: maximum action = stay, $v'(s=2) = r + \gamma v(2) = 0$
- State 3: maximum action = stay, $v'(s=3) = r + \gamma v(3) = 0$
- State 4: maximum action = stay, $v'(s=4) = r + \gamma v(4) = 0$
- state 5: maximum action = stay, $v'(s=5) = r + \gamma v(5) = 0$
- state 6: maximum action = right, $v'(s=6) = r + \gamma v(7) = 10$

Now the value function looks like so:

![Value Function, Second Iteration](/images/blog_pics/RL1/fig2.png)

Let's go ahead and do the third iteration:

- State 2: maximum action = stay, $v'(s=2) = r + \gamma v(2) = 0$
- State 3: maximum action = stay, $v'(s=3) = r + \gamma v(3) = 0$
- State 4: maximum action = stay, $v'(s=4) = r + \gamma v(4) = 0$
- state 5: maximum action = right, $v'(s=5) = r + \gamma v(6) = 10$
- state 6: maximum action = right, $v'(s=6) = r + \gamma v(7) = 10$

The value function is now:

![Value Function, Third Iteration](/images/blog_pics/RL1/fig3.png)

Notice how information of state seven's reward of +10 propogates backwards to the other states. For this reason, line 7 in the above algorithm is sometimes called the Bellman Backup Operation. Every iteration, the value function we're computing gets closer to the optimal solution, and it does so by ``backing up'' the information of rewards to all other states. 

The program terminates on the optimal value function:

![Value Function, Final](/images/blog_pics/RL1/fig4.png)

The optimal value function above shows that every state can achieve a high reward. Given this value function, what is the optimal policy we should follow? That is simple: choose the action that maximizes $r + \gamma v_*(s')$. In our 1D grid world, the we would walk to the right in every state (except state 1, where we're doomed anyway).

There is a second iterative to find the optimal policy, called policy iteration.

Policy Iteration
======

As the name suggests, policy iteration iterates through policies until it converges on the optimal policy. Below is the algorithm:

1. Start with model $p(s',r\|s,a)$
2. Initialize policy $\pi(a\|s)$ for all states $s$
3. Initialize current value function $v(s)=0$ for all states
4. do:
5. &emsp; for $s\in S$:
6. &emsp; &emsp; $v(s) = v(s')$
7. &emsp; &emsp; $v'(s) = \sum _{s'\in S, a \in A} P(s',r\|s,a)\pi(a\|s)\left( r + \gamma v(s') \right)$
8. while $\|v'(s) - v(s)\|_1 < \text{small number}$
9. for $s\in S$:
10. &emsp; 

9. return v(s)